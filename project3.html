<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Athina Stewart - Software Engineer</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<!-- <a href="index.html" class="logo">
									<span class="symbol"><img src="images/logo.svg" alt="" /></span><span class="title">Phantom</span>
								</a> -->

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="index.html">Home</a></li>
							<li><a href="aboutme.html">About Me</a></li>
							<li><a href="https://drive.google.com/file/d/1Hykz9E4BPszkW3bLhDe0NqEt3TWZ68xt/view?usp=sharing">Resume</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Quantization and Pruning of the BERT model</h1>
							<span class="image main"><img src="images/christina-wocintechchat-com-LQ1t-8Ms5PY-unsplash (1).jpg" alt="" /></span>

							<div class="center-box">
								<h2>End to End Data Science Project | Python, Google Places API, Selenium</h2>
							</div>

							<div id="text-wrapper">

								<h4> Abstract</h4>
								<p>
									This project addresses the challenges in natural language understanding posed by the ambiguity and 
									complexity of language. Focusing on Bidirectional Encoder Representations from Transformers 
									(BERT), the study explores modifications such as pruning and quantization to enhance BERT's 
									performance on General Language Understanding Evaluation (GLUE) benchmark tasks. BERT's 
									bidirectional context understanding is crucial for language comprehension. The experiment 
									hypothesizes that pruning and quantization will improve validation accuracy, considering 
									potential benefits in model compression. Results demonstrate promising outcomes, with quantization 
									effectively reducing model size without compromising accuracy. The study opens avenues for further 
									exploration, including alternative pruning techniques and comparisons across language models, 
									emphasizing the importance of ongoing research in optimizing compression strategies for advanced 
									NLP models like BERT.
								</p>

                                The full paper for this project can be found <a href="https://drive.google.com/file/d/1WpmSys9bx4c8eKIp97KCL4m1KXh25h4h/view?usp=sharing" target="_blank">here</a>
                                <br>
                                Supporting Code: <a href="https://github.com/athina-stewart/Pruning-and-Quantization-on-BERT-Model" target="_blank">here</a>  
                                <br>
                                <br>

								<h4> The Problem</h4>
								<p>   
                                    Language, with its inherent ambiguities, polysemous nature, and synonyms, poses significant challenges 
                                    in natural language understanding. Words often carry multiple meanings, further complicated by homophones 
                                    and the nuances of prosody. The meaning of a word evolves within a specific context, emphasizing the 
                                    necessity of context and common sense reasoning for effective language comprehension.

                                    <br>
                                    <br>

                                    Traditional natural language processing (NLP) models, including those based on recurrent neural networks 
                                    (RNNs) and convolutional neural networks (CNNs), struggled to capture the intricate contextual relationships 
                                    in language. Bidirectional Encoder Representations from Transformers (BERT) emerged as a solution to address 
                                    these limitations. Unlike unidirectional models, BERT introduced bidirectional context understanding, enabling 
                                    the consideration of both left and right context for each word, resulting in enhanced contextual representation.

                                    <br>
                                    <br>

                                    To delve deeper into the challenges and potential enhancements, this report explores modifications such as 
                                    pruning and quantization applied to the BERT model. The objective is to fine-tune the model on datasets from 
                                    the General Language Understanding Evaluation (GLUE) benchmark, a diverse collection of language understanding 
                                    tasks. The hypothesis posits that a quantized or pruned model will yield an improvement in validation accuracy. 
                                    The experiments focus on specific tasks within the GLUE benchmark, including sentence pair tasks (MRPC) and 
                                    single sentence classification tasks (SST-2 and CoLA).
                                </p>

								<h4> The BERT Model</h4>
								<p>   
                                    <!-- <i>The BERT Model</i> -->
                                    <!-- <br>
                                    <br> -->

                                    The BERT-Base model, developed by Google, represents a cutting-edge natural language processing (NLP) model 
                                    with 12 transformers and 110 million parameters. Key features include its foundation on the Transformer 
                                    architecture, bidirectional training for comprehensive context understanding, and pre-training on large text 
                                    datasets in an unsupervised manner. BERT's contextual embeddings capture nuanced word representations by 
                                    considering the surrounding context in a sentence. It employs training strategies such as Masked Language 
                                    Model (MLM) and Next Sentence Prediction (NSP) to enhance contextual awareness. BERT's bidirectional context 
                                    understanding significantly improves its ability to grasp complex contextual relationships in language.
								</p>

								<h4> Fine-Tuning the Model</h4>
								<p>
                                    <!-- <br>
                                    <br>
                                    <i>Fine-Tuning the Model</i>
                                    <br>
                                    <br> -->

                                    Fine-tuning BERT involves utilizing its pre-trained weights on a large corpus and adapting it to specific 
                                    downstream tasks. The model is pre-trained on extensive datasets like BooksCorpus and English Wikipedia. 
                                    Fine-tuning allows incorporating task-specific data, expanding the vocabulary, and achieving better accuracies. 
                                    BERT proves versatile for various NLP tasks, including question-answering, sentiment analysis, and document 
                                    classification. The fine-tuning process involves using BERT-Base on different NLP tasks, adjusting parameters 
                                    like batch size and sequence length, and leveraging transfer learning for efficient model development.
								</p>

								<h4> Proposed Modifications</h4>
								<p>

                                    <!-- <br>
                                    <br>
                                    <i>Proposed Modifications</i>
                                    <br>
                                    <br> -->

                                    Despite BERT's success, overparameterization poses challenges, especially with limited training examples. 
                                    To address overfitting, the report explores network compression through quantization and pruning. Quantization 
                                    reduces model precision, while pruning removes certain connections, leading to a sparser model. These methods 
                                    aim to enhance model efficiency and mitigate overfitting, offering potential improvements in model performance. 
                                    The study focuses on quantization and pruning as regularization techniques to refine the BERT model for 
                                    downstream tasks, ensuring simplicity and explainability in the results.
								</p>

								<h4> Experiment Hypothesis and Design</h4>
								<p>

                                    <!-- <br>
                                    <br>
                                    <i>Experiment Hypothesis and Design</i>
                                    <br>
                                    <br> -->

                                    The experiment aims to assess the impact of regularization techniques, specifically pruning and quantization, 
                                    on BERT's performance in downstream tasks. The hypothesis posits that these techniques will mitigate overfitting, 
                                    leading to increased validation accuracy. The experiment encompasses three conditions: (i) without pruning and 
                                    quantization, (ii) with pruning, and (iii) with quantization. Quantization involves reducing model precision by 
                                    adopting more compact formats for weights, such as integers or binary numbers. Dynamic quantization, chosen for 
                                    its simplicity and effectiveness, dynamically quantizes weights during inference without retraining. Pruning, on 
                                    the other hand, entails removing less important parameters to create a more compact and efficient model. The 
                                    experiment utilizes magnitude-based pruning, where parameters with small magnitudes are removed.
								</p>

								<h4> Experiment Methodology</h4>
								<p>

                                    <!-- <br>
                                    <br>
                                    <i>Experiment Methodology</i>
                                    <br>
                                    <br> -->

                                    Quantization employs PyTorch's dynamic quantization library, specifically the <code>torch.quantization.quantize_dynamic </code>
                                    function, to reduce model precision during runtime. The quantization process dynamically reduces weights and 
                                    activations to 8 bits. The initial size of the BERT model is measured in terms of memory consumption. The dataset, 
                                    including SST-2 and MRPC, is downloaded and preprocessed for the experiment. Model configuration involves specifying 
                                    the pre-trained BERT model, maximum sequence length, GLUE task, and other parameters. Device and batch configuration 
                                    set the device, batch size, and other relevant parameters. The experiment uses random seed 42 for reproducibility.
								</p>

								<h4> Methodology</h4>
								<p>

                                    <!-- <br>
                                    <br>
                                    <i>Experiment Analysis and Methodology Extension</i>
                                    <br>
                                    <br> -->

                                    The experiment involves measuring the sparsity level and resulting model size after quantization, along with 
                                    performance metrics like accuracy and loss after fine-tuning. Pruning parameters include initial and final sparsity, 
                                    begin and end steps for the pruning schedule. Quantization parameters involve the precision reduction amount for 
                                    model weights and activations. Control variables include BERT architecture, downstream task and dataset, and 
                                    evaluation metrics. The experiment methodology extends to model pruning, where the BERT preprocessing model tokenizes 
                                    input sentences, and magnitude pruning is applied to the dense layer. The model is compiled and trained, with a 
                                    pruning callback updating pruning masks during training. The BERT model is fine-tuned on Cola and MRPC tasks as part 
                                    of the experiment.
								</p>

								<h4> Experiment Results</h4>
								<p>

									<!-- <br>
                                    <br>
                                    <i>Experiment Results</i>
                                    <br>
                                    <br> -->

									Challenges encountered during the experiment include TensorFlow quantization issues, determining optimal hyperparameters
									for pruning, and handling data preprocessing for GLUE benchmark datasets. Switching from TensorFlow to PyTorch for
									quantization provided more flexibility. Identifying hyperparameters for pruning involved extensive experimentation, 
									with considerations like initial sparsity, final sparsity, begin_step, and end_step.

									<br>
                                    <br>

									Performance metrics presented in Tables 1, 2, and 3 showcase the outcomes of pruning and quantization experiments. Pruning, 
									fine-tuned on the MRPC and CoLA datasets, exhibited a decrease in training loss and an increase in training accuracy over 
									epochs, suggesting potential benefits in terms of model compression. The quantized BERT model, fine-tuned on MRPC and SST-2, 
									demonstrated comparable accuracy to non-quantized models, emphasizing the effectiveness of quantization in reducing model 
									size without compromising performance. The quantized models also exhibited a significant reduction in model size (58.62%). 
									Preliminary results from fine-tuning the BERT model pre-quantization and pruning on the CoLA and MRPC datasets show trends 
									in accuracy and loss, providing insights into the model's behavior during training and validation.

									<br>
                                    <br>

									In future work, the experimenters propose exploring additional pruning and quantization techniques, such as neuron pruning, 
									sparse pruning, and structured pruning, to determine their effectiveness for the BERT model. Comparisons across different 
									language models could offer broader insights into the generalizability of these compression strategies on various architectures. 
									The preliminary results hint at nuances in validation accuracy and loss, emphasizing the importance of continued exploration 
									and optimization of compression techniques for BERT.

									<div class="col-12"><span class="image fit"><img src="images/BERT results.png" alt="" /></span></div>
                                </p>
							</div>
						</div>
					</div>

				<!-- Footer -->
				<footer id="footer">
					<div class="inner">
						<section>
							<h2>Let's Connect!</h2>
							<p>Please get in touch with me by either sending an email or connecting with me on LinkedIn</p>
							<!-- <form method="post" action="#">
								<div class="fields">
									<div class="field half">
										<input type="text" name="name" id="name" placeholder="Name" />
									</div>
									<div class="field half">
										<input type="email" name="email" id="email" placeholder="Email" />
									</div>
									<div class="field">
										<textarea name="message" id="message" placeholder="Message"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send" class="primary" /></li>
								</ul>
							</form> -->
						</section>
						<section>
							<h2>Follow</h2>
							<ul class="icons">
								<li><a href="https://github.com/athina-stewart" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="mailto:athina.stewart@gmail.com" class="icon solid style2 fa-envelope"><span class="label">Email</span></a></li>
								<li><a href="https://www.linkedin.com/in/athinastewart/" class="icon brands style2 fa-linkedin"><span class="label">LinkedIn</span></a></li>
							</ul>
						</section>
						<ul class="copyright">
							<li>&copy; Untitled. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</div>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>